%% Based on a TeXnicCenter-Template by Tino Weinkauf.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% HEADER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,twoside,10pt]{report}
% Alternative Options:
%	Paper Size: a4paper / a5paper / b5paper / letterpaper / legalpaper / executivepaper
% Duplex: oneside / twoside
% Base Font Size: 10pt / 11pt / 12pt


%% Language %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[USenglish]{babel} %francais, polish, spanish, ...
\usepackage[T1]{fontenc}
\usepackage[ansinew]{inputenc}

\usepackage{lmodern} %Type1-font for non-english texts and characters


%% Packages for Graphics & Figures %%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx} %%For loading graphic files
%\usepackage{subfig} %%Subfigures inside a figure
%\usepackage{tikz} %%Generate vector graphics from within LaTeX

%% Please note:
%% Images can be included using \includegraphics{filename}
%% resp. using the dialog in the Insert menu.
%% 
%% The mode "LaTeX => PDF" allows the following formats:
%%   .jpg  .png  .pdf  .mps
%% 
%% The modes "LaTeX => DVI", "LaTeX => PS" und "LaTeX => PS => PDF"
%% allow the following formats:
%%   .eps  .ps  .bmp  .pict  .pntg


%% Math Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}

%% Line Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{setspace}
%\singlespacing        %% 1-spacing (default)
%\onehalfspacing       %% 1,5-spacing
%\doublespacing        %% 2-spacing


%% Other Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{a4wide} %%Smaller margins = more text per page.
%\usepackage{fancyhdr} %%Fancy headings
%\usepackage{longtable} %%For tables, that exceed one page


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Remarks
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% TODO:
% 1. Edit the used packages and their options (see above).
% 2. If you want, add a BibTeX-File to the project
%    (e.g., 'literature.bib').
% 3. Happy TeXing!
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Options / Modifications
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\input{options} %You need a file 'options.tex' for this
%% ==> TeXnicCenter supplies some possible option files
%% ==> with its templates (File | New from Template...).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% DOCUMENT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\pagestyle{empty} %No headings for the first pages.


%% Title Page %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ==> Write your text here or include other files.

%% The simple version:
\title{PVM Algorithm Pseudocode}
\author{Andrei Sucil\u a}
%\date{} %%If commented, the current date is used.
\maketitle

%% The nice version:
%\input{titlepage} %%You need a file 'titlepage.tex' for this.
%% ==> TeXnicCenter supplies a possible titlepage file
%% ==> with its templates (File | New from Template...).


%% Inhaltsverzeichnis %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents %Table of contents
\cleardoublepage %The first chapter should start on an odd page.

\pagestyle{plain} %Now display headings: headings / fancy / ...



%% Chapters %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ==> Write your text here or include other files.

%\input{intro} %You need a file 'intro.tex' for this.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ==> Some hints are following:

\chapter{Introduction}\label{intro}


\section{List of Adnotations}\label{adnote_list}

	\begin{itemize}
		\item \(\mathbb{R}^n\) the \(n\)-dimensional real space
		\item \(H\) a Hilbert space
		\item \(<x_0, x_1>\) the scalar product for \(x_0, x_1\in H\)
		\item \(x_i \in \mathbb{R}^n\) a general point in \(n\) dimensions with \(y_i\in \{\pm 1\} \}\) its label
		\item \(\overline{a}\), where \(a\in\mathbb{R}^n\), is the unit vector corresponding to \(a\)
		\item \(<x_0, x_1>\), where \(x_0, x_1 \in \mathbb{R}^n\), is the scalar product
		\item \(S = \{(x_i, y_i) | x_i \in \mathbb{R}^n, y_i\in \{\pm 1\} \}, i\in \overline{1..m}\) - the initial training set
		\item \(S_+ = \{x_i\in \mathbb{R}^n | y_i = 1 \} \), \(S_- = \{x_i\in \mathbb{R}^n | y_i = -1 \} \) the positively labeled subset of 
							training points and the negatively labeled subset of training points
		\item \(b\in \mathbb{R}\) the bias of the separating hyperplane
		\item \(\omega \in \mathbb{R}^n\), if no kernel is used, this is the normal vector to the separating hyperplane; the equation for the separating 
							hyperplane is thus expressed as :
							\begin{itemize}
								\item \(<\omega, x> + b \geq 0\) for the positive side of the separation
								\item \(<\omega, x> + b \leq 0\) for the negative side of the separation
							\end{itemize}
		\item \(\alpha_i \in \mathbb{R}\), where \(i\in\overline{1..m}\) the kernel members coefficients for when a kernel is used; the equation for the separating 
							hyperplane is thus expressed as :
							\begin{itemize}
								\item \(\sum^m_{i=1}{\alpha_i K(x_i, x)} + b \geq 0\) for the positive side of the separation
								\item \(\sum^m_{i=1}{\alpha_i K(x_i, x)} + b \leq 0\) for the negative side of the separation
								%mcu: aici am pus eu sum^m in loc de sum^n -- sunt m items conform cu ce ai zis tu mai sus
								%ai dreptate aici
							\end{itemize}
							
		\item	\(A\in M_{m\times n}(\mathbb{R})\) an \(m\times n\) (\(m\)lines, \(n\) columns) matrix with real coefficients
	\end{itemize}
	
	
	
\chapter{Pseudocodes}
	
\section{PVM Problem}\label{pvm_feas}

\subsection{Initial Formulation}

	The formulation for the PVM Feasibility Problem(PVMFP), parametrized by \( t \in \mathbb{R}_+ \), will be:
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				\frac{1}{|S_+|} \sum_{x\in S_+} {(<w,x> + b)} = {E_+}\\
				\frac{1}{|S_-|} \sum_{x\in S_-} {(<w,x> + b)} = -{E_-}\\
				
				|(<w,x_i> + b) - E_+| \leq \sigma_+^i, x_i\in S_+\\
				|(<w,x_i> + b) + E_-| \leq \sigma_-^i, x_i\in S_-\\
				
				\frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {\sigma_+^i} = {\sigma_+} \\ %mcu: aici nu ai zis nicaieri cine este \sigma_+ -> este o necunoscuta in sistem, introdusa prin egalitatea asta
				\frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {\sigma_-^i} = {\sigma_-} \\ %corespunde deviatiei medii a distantelor, dar oricum va fi eliminata in ver de mai jos a sistemului
				
				\sigma_+ \leq t \cdot E_+\\
				\sigma_- \leq t \cdot E_-\\
				E_+ > 0\\
				E_- > 0\\
		\end{array}
		\right. \label{feas_0}
	\]
	
	The kernel formulation for the PVMFP, parametrized by \( t \in \mathbb{R}_+ \), will be:
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				b + \sum_{x_i\in S} {\alpha_i \cdot \frac{1}{|S_+|}\sum_{x_j\in S_+} {K(x_i, x_j)}} = E_+\\
				b + \sum_{x_i\in S} {\alpha_i \cdot \frac{1}{|S_-|}\sum_{x_j\in S_-} {K(x_i, x_j)}} = E_-\\
				
				|\sum_{x_i\in S} {\alpha_i K(x_i, x_j)} + b - E_+| \leq \sigma^j_+ \mbox{ , for each } x_j \in S_+\\
				|\sum_{x_i\in S} {\alpha_i K(x_i, x_j)} + b + E_-| \leq \sigma^j_- \mbox{ , for each } x_j \in S_-\\
					
				\frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {\sigma_+^i} = {\sigma_+} \\
				\frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {\sigma_-^i} = {\sigma_-} \\
				
				\sigma_+ \leq t \cdot E_+\\
				\sigma_- \leq t \cdot E_-\\
				E_+ > 0\\
				E_- > 0\\
		\end{array}
		\right. \label{feas_1}
	\]	
	
	In order to rewrite the above, let us denote by:
	
	\[
		\begin{array}{l}
			K^i_+ = \frac{1}{|S_+|}\sum_{x_j \in S_+} {K(x_i, x_j)} \\
			K^i_- = \frac{1}{|S_-|}\sum_{x_j \in S_-} {K(x_i, x_j)} \\
		\end{array}
	\]
	
	By integrating the equalities, the system then becomes:
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				|\sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)}| \leq \sigma^j_+ \mbox{ , for each } x_j \in S_+\\
				|\sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)}| \leq \sigma^j_+ \mbox{ , for each } x_j \in S_+\\								
				
				\frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {\sigma_+^i} \leq t \cdot (b + \sum_{x_i\in S}{\alpha_i K^i_+})\\
				\frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {\sigma_-^i} \leq t \cdot (-b - \sum_{x_i\in S}{\alpha_i K^i_-})\\
				b + \sum_{x_i\in S}{\alpha_i K^i_+} > 0\\
				- b - \sum_{x_i\in S}{\alpha_i K^i_+} > 0\\
		\end{array}
		\right. \label{feas_2}
	\]	
	
	We can rewrite this in a standardized form as:
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				\sigma^j_+ - (\sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)}) \geq 0 \mbox{ , for each } x_j \in S_+\\
				\sigma^j_+ + \sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)} \geq 0 \mbox{ , for each } x_j \in S_+\\
				
				\sigma^j_- - (\sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_-)}) \geq 0 \mbox{ , for each } x_j \in S_-\\
				\sigma^j_- + \sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_-)} \geq 0 \mbox{ , for each } x_j \in S_-\\									
				
				t \cdot (b + \sum_{x_i\in S}{\alpha_i K^i_+}) - \frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {\sigma_+^i} \geq 0\\
				t \cdot (- b - \sum_{x_i\in S}{\alpha_i K^i_-}) - \frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {\sigma_-^i} \geq 0\\
				b + \sum_{x_i\in S}{\alpha_i K^i_+} > 0\\
				- b - \sum_{x_i\in S}{\alpha_i K^i_+} > 0\\
				\epsilon \geq 0\\
		\end{array}
		\right. \label{feas_3}
	\]		
	
	The system \ref{feas_3} is the system that will be passed to the feasibility solver.
	
\subsection{Adding Weights}	
	
	It may be necessary that we add weights to points. One such example would be when, prior to training, a clustering on the initial data 
is undertaken in order to minimize the number of training points, by replacing them with the centroids of the clusters. In this case, we would want 
a centroid to corectly represent its cluster by contributing to the average and average deviation proportionally to the number of elements represented in the cluster.

	The following notations shall be used: 
	
	\begin{itemize}
	 \item \(p_i \in [1, \infty)\) shall denote the weight of \(x_i\). 
	 \item \(|S_+| =\sum_{i : x_i\in S_+}{p_i}\) the sum of weights coresponding to the positively labeled points
	 \item \(|S_-| =\sum_{i : x_i\in S_-}{p_i}\) the sum of weights coresponding to the negatively labeled points
	 \item \(K^i_+ = \frac{1}{|S+|} \sum_{x_j\in S_+}{p_j K(x_i, x_j)}\)
	 \item \(K^i_- = \frac{1}{|S-|} \sum_{x_j\in S_-}{p_j K(x_i, x_j)}\)
	\end{itemize}
	
	Note that, if \(S_+\) has at least 2 members, then \(|S_+| > 1\) and thus \(|S_+| - 1 > 0\). Similarly, \(|S_-| - 1 > 0\).
	
	The feasibility problem coresponding to \(t\in\mathbb{R}_+\) becomes :
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				\frac{1}{|S_+|} \sum_{x_i\in S_+} {p_i(<w,x_i> + b)} = {E_+}\\
				\frac{1}{|S_-|} \sum_{x_i\in S_-} {p_i(<w,x_i> + b)} = -{E_-}\\
				
				|(<w,x_i> + b) - E_+| \leq \sigma_+^i, x_i\in S_+\\
				|(<w,x_i> + b) + E_-| \leq \sigma_-^i, x_i\in S_-\\
				
				\frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {p_i \sigma_+^i} = {\sigma_+} \\ %mcu: aici nu ai zis nicaieri cine este \sigma_+ -> este o necunoscuta in sistem, introdusa prin egalitatea asta
				\frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {p_i \sigma_-^i} = {\sigma_-} \\ %corespunde deviatiei medii a distantelor, dar oricum va fi eliminata in ver de mai jos a sistemului
				
				\sigma_+ \leq t \cdot E_+\\
				\sigma_- \leq t \cdot E_-\\
				E_+ > 0\\
				E_- > 0\\
		\end{array}
		\right. \label{feas__weight_0}
	\]
	
	Introducing the kernel, the system becomes : 
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				b + \sum_{x_i\in S} {\alpha_i \cdot [\frac{1}{|S_+|}\sum_{x_j\in S_+} {p_j K(x_i, x_j)}]} = E_+\\
				b + \sum_{x_i\in S} {\alpha_i \cdot [\frac{1}{|S_-|}\sum_{x_j\in S_-} {p_j K(x_i, x_j)}]} = -E_-\\
				
				|\sum_{x_i\in S} {\alpha_i K(x_i, x_j)} + b - E_+| \leq \sigma^j_+ \mbox{ , for each } x_j \in S_+\\
				|\sum_{x_i\in S} {\alpha_i K(x_i, x_j)} + b + E_-| \leq \sigma^j_- \mbox{ , for each } x_j \in S_-\\
					
				\frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {p_i \sigma_+^i} = {\sigma_+} \\
				\frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {p_i \sigma_-^i} = {\sigma_-} \\
				
				\sigma_+ \leq t \cdot E_+\\
				\sigma_- \leq t \cdot E_-\\
				E_+ > 0\\
				E_- > 0\\
		\end{array}
		\right. \label{feas_weight_1}
	\]	

	By using the agreed upon notation, the system becomes:
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				b + \sum_{x_i\in S} {\alpha_i \cdot K^i_+} = E_+\\
				b + \sum_{x_i\in S} {\alpha_i \cdot K^i_-} = -E_-\\
				
				|\sum_{x_i\in S} {\alpha_i K(x_i, x_j)} + b - E_+| \leq \sigma^j_+ \mbox{ , for each } x_j \in S_+\\
				|\sum_{x_i\in S} {\alpha_i K(x_i, x_j)} + b + E_-| \leq \sigma^j_- \mbox{ , for each } x_j \in S_-\\
					
				\frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {p_i \sigma_+^i} = {\sigma_+} \\
				\frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {p_i \sigma_-^i} = {\sigma_-} \\
				
				\sigma_+ \leq t \cdot E_+\\
				\sigma_- \leq t \cdot E_-\\
				E_+ > 0\\
				E_- > 0\\
		\end{array}
		\right. \label{feas_weight_2}
	\]	
	
	
	Replacing the equalities in the other constraints, the system becomes:
 

	\[
		Feas(t) = \left\{
		\begin{array}{l}			
							
				|\sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)}| \leq \sigma^j_+ \mbox{ , for each } x_j \in S_+\\
				|\sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_-)}| \leq \sigma^j_- \mbox{ , for each } x_j \in S_-\\
									
				t(|S_+| - 1)\sum_{i : x_i \in S}{\alpha_i \cdot K^i_+} + t(|S_+| - 1)b - \sum_{x_i\in S_+}{p_i \sigma^i_+} \geq 0\\
				t(|S_-| - 1)\sum_{i : x_i \in S}{\alpha_i \cdot (-K^i_-)} - t(|S_-| - 1)b - \sum_{x_i\in S_-}{p_i \sigma^i_-} \geq 0\\								
				
				b + \sum_{x_i\in S} {\alpha_i \cdot K^i_+} > 0\\
				-b + \sum_{x_i\in S} {\alpha_i \cdot (-K^i_-)} > 0\\				
		\end{array}
		\right. \label{feas_weight_3}
	\]	
	
	This last form, \ref{feas_weight_3}, is the system that is to be solved by the feasibility solver, with unknowns \(\alpha_i, \sigma^i_+, \sigma^i_-\) and \(b\). 
	
	
\subsection {Feasibility System Formulation Pseudocode}	

	The pseudocode for formulating the feasibility system shall be detailed in this subsection.
	
	
	First, the computation of \(K^i_+\) 
	
	\begin{algorithm}
	
	\caption{Computation of \(K^i_+\)}
	\label{comp_ki_plus}		
	
	\begin{center}
	\begin{algorithmic}	
	
		\STATE Input \(x_i\)
		\STATE \(ReturnValue \gets 0\)
		
		\FOR {(\(x_j = S_+\).begin; \(x_j < S_+\).end; \(x_j++\))}
			\STATE ReturnValue += \(p_j \cdot K(x_i, x_j)\)
		
		\ENDFOR
		
		\STATE ReturnValue /= \(|S_+|\)
		
		\STATE return ReturnValue
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}	

	and \(K^i_-\):
	
	
	\begin{algorithm}
	
	\caption{Computation of \(K^i_-\)}
	\label{comp_ki_minus}		
	
	\begin{center}
	\begin{algorithmic}	
	
		\STATE Input \(x_i\)
		\STATE \(ReturnValue \gets 0\)
		
		\FOR {(\(x_j = S_-\).begin; \(x_j < S_-\).end; \(x_j++\))}
			\STATE ReturnValue += \(p_j \cdot K(x_i, x_j)\)
		
		\ENDFOR
		
		\STATE ReturnValue /= \(|S_-|\)
		
		\STATE return ReturnValue
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}	


	For the formulation of system \ref{feas_weight_3}, the following procedures will be considered as already defined:
	
	\begin{itemize}
		\item Indexing of all the unknown variables. The index of a variable, \(var\), will be given by \(idx(var)\) (e.g. \(idx(\alpha_i)\)). Indexes will start from 0 and end with the number of unknowns.
		\item The init procedure for a new constraint, \(cons\), will be given by \(cons = NewConstraint()\). 
		\item The \(clear\) procedure for a constraint will clear it of its current contents. 
		\item A constraint will also have the \(set\) function which, for a given index, sets the value of the coefficient. e.g. \(cons.set(idx(\alpha_i), val)\) sets the coefficient for \(\alpha_i\) in \(cons\) to value \(val\). For the indexes for which the \(set\) function has not been called, the value will implicitly be 0. 
		\item A constraint will also have the \(get\) function which, for a given index, returns the value for the coefficient of that index, e.g. \(cons.get(idx(\alpha_i))\) returns the value of the coefficient set for \(\alpha_i\), or 0 in case that that coefficient has not yet been set.
		\item Denote by \(FeasSys\) the currently constructed feasibility system. Denote by \(PushBack\) the procedure of adding a constraint of a type, with a right hand value, to the system by copying it e.g. \(FeasSys.PushBack(cons, GreaterEqual, RHV)\).
	\end{itemize}
	
	The algorithm for constructing the feasibility system \ref{feas_weight_3} is split in three parts.
	
	The first part introduces the deviations from the averages:	
	
	\begin{algorithm}
	
	\caption{Feasibility System Construction Part 1}
	\label{feassystemconstruction1}		
	
	\begin{center}
	\begin{algorithmic}	
	
	\STATE Input : \(t\in \mathbb{R}_+, S_+, S_-\)
	
	%aici se adauga constrangerile care definesc deviatiile de la media pozitiva a pct cu label pozitiv:
	
	\STATE cons = NewConstraint()
	\FOR {\(x_j = S_+.begin\); \(x_j < S_+.end\); \(x_j ++\)}
		%adaugam prima constrangere pentru definirea inegalitatii cu modul : sigma^j_+ >= Sum(alpha_i * (K(x_i, x_j) - K^i_+))
		%forma echivalenta: sigma^j_+ + Sum(alpha_i * (K^i_+ - K(x_i, x_j))) >= 0
		\STATE cons.Clear()
		
		\FOR{(i = 0; i < S.count; i++)}
			\STATE \(cons.set(idx(\alpha_i), K^i_+ - K(x_i, x_j))\)
		\ENDFOR
		
		\STATE \(const.set(idx(\sigma^j_+), 1.0)\)
		\STATE \(FeasSys.PushBack(cons, GreaterEqual, 0)\)
		
		%adaugam prima constrangere pentru definirea inegalitatii cu modul : sigma^j_+ >= -Sum(alpha_i * (K(x_i, x_j) - K^i_+))
		%forma echivalenta: sigma^j_+ + Sum(alpha_i * (K(x_i, x_j) - K^i_+)) >= 0
		%observa ca forma aceasta are doar semnele schimbate pt coef alpha_i			
		\FOR{(i = 0; i < S.count; i++)}
			\STATE \(cons.set(idx(\alpha_i), - cons.get(idx(\alpha_i)))\)
		\ENDFOR
				
		\STATE \(FeasSys.PushBack(cons, GreaterEqual, 0)\)		
		
	\ENDFOR
	
	%aici se adauga constrangerile care definesc deviatiile de la media negativa a pct cu label negativ:
	\FOR {\(x_j = S_-.begin\); \(x_j < S_-.end\); \(x_j ++\)}
		%adaugam prima constrangere pentru definirea inegalitatii cu modul : sigma^j_- >= Sum(alpha_i * (K(x_i, x_j) - K^i_-))
		%forma echivalenta: sigma^j_- + Sum(alpha_i * (K^i_- - K(x_i, x_j))) >= 0
		\STATE cons.Clear()
		
		\FOR{(i = 0; i < S.count; i++)}
			\STATE \(cons.set(idx(\alpha_i), K^i_- - K(x_i, x_j))\)
		\ENDFOR
		
		\STATE \(cons.set(idx(\sigma^j_-), 1.0)\)
		\STATE \(FeasSys.PushBack(cons, GreaterEqual, 0)\)
		
		%adaugam prima constrangere pentru definirea inegalitatii cu modul : sigma^j_- >= -Sum(alpha_i * (K(x_i, x_j) - K^i_-))
		%forma echivalenta: sigma^j_- + Sum(alpha_i * (K(x_i, x_j) - K^i_-)) >= 0
		%observa ca forma aceasta are doar semnele schimbate pt coef alpha_i			
		\FOR{(i = 0; i < S.count; i++)}
			\STATE \(cons.set(idx(\alpha_i), - cons.get(idx(\alpha_i)))\)
		\ENDFOR
				
		\STATE \(FeasSys.PushBack(cons, GreaterEqual, 0)\)		
	\ENDFOR
		
	\STATE delete cons
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}	
	
	
	
	The second part imposes the inequality between the average deviations and the averages:
	
	\begin{algorithm}
	
	\caption{Feasibility System Construction Part 2}
	\label{feassystemconstruction2}		
	
	\begin{center}
	\begin{algorithmic}	
	
	\STATE cons = NewConstraint()
	
	%adaug constrangerea pentru media pozitiva si deviatia medie pozitiva
	%  t(|S_+| - 1)\sum{\alpha_i K^i_+} + t(|S_+| - 1)b - \sum{\sigma^i_+ p_i}
	\FOR{(i = 0; i < S.count; i++)}
		\STATE \(cons.set(idx(\alpha_i), t(|S_+| - 1)K^i_+)\)		
	\ENDFOR
	
	\STATE\(cons.set(idx(b), t(|S_+| - 1)\)
	
	\FOR{\((x_i = S_+.begin; x_i < S_+.end; x_i++)\)}
		\STATE \(cons.set(idx(\sigma^i_+), -p_i)\)
	\ENDFOR
	
	\STATE FeasSys.PushBack(cons, GreaterEqual, 0)
	
	\STATE cons.Clear()
	%adaug constrangerea pentru media pozitiva si deviatia medie pozitiva
	%  t(|S_+| - 1)\sum{\alpha_i K^i_+} + t(|S_+| - 1)b - \sum{\sigma^i_+ p_i}	
	\FOR{(i = 0; i < S.count; i++)}
		\STATE \(cons.set(idx(\alpha_i), -t(|S_-| - 1)K^i_-)\)
	\ENDFOR
	
	\STATE\(cons.set(idx(b), -t(|S_-| - 1)\)
	
	\FOR{\((x_i = S_-.begin; x_i < S_-.end; x_i ++)\)}
		\STATE \(cons.set(idx(\sigma^i_-), -p_i)\)
	\ENDFOR
	
	\STATE FeasSys.PushBack(cons, GreaterEqual, 0)
	
	\STATE delete cons
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}	
	
	\clearpage
	
	The last part constrains the averages to be strictly positive. Remember that this is a necessary condition without which the system would accept a null solution, making the classification meaningless. 
	We will express these two conditions by introducing an extra variable, \(\epsilon\), from which we will ask to be smaller than the averages, but positive. This has the benefit of keeping the system to the form \(Ax \geq 0\), which can then be rescalled as we see fit during the solving stage.
	
	\begin{algorithm}
	
	\caption{Feasibility System Construction Part 3}
	\label{feassystemconstruction3}		
	
	\begin{center}
	\begin{algorithmic}		
	
	\STATE cons = NewConstraint()
	
	\FOR {(i = 0; i < S.count; i++)}
		\STATE \(cons.set(idx(\alpha_i), K^i_+)\)
	\ENDFOR
	
	\STATE \(cons.set(idx(b), 1.0)\)
	\STATE \(cons.set(idx(\epsilon), -1.0)\)
	
	\STATE FeasSyS.PushBack(cons, GreaterEqual, 0)
	
	
	\STATE cons.Clear()
	
	\FOR{(i = 0; i < S.count; i++)}
		\STATE \(cons.set(idx(\alpha_i), -K^i_-)\)
	\ENDFOR
	
	\STATE \(cons.set(idx(b), -1.0)\)
	\STATE \(cons.set(idx(\epsilon), -1.0)\)
	
	\STATE FeasSys.PushBack(cons, GreaterEqual, 0)
	
	\STATE cons.Clear()
	
	\STATE \(cons.set(idx(\epsilon), 1.0)\)
	
	\STATE FeasSys.PushBack(cons, Greater, 0)
	
	\STATE delete cons
	
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}	
	\clearpage
			
\subsection{The Binary Search for Solving PVM}

	Solving the PVM classification problem consists of finding:
	
	\begin{enumerate} 
		\item \(t_{optimal}\in\mathbb{R}_+\) 
		\item \begin{itemize}
						\item \(\omega\in\mathbb{R}^n, b\in\mathbb{R}\) for non-kernel classification
						\item \(\alpha _i \in \mathbb{R}, i\in\overline{1..m}, b\in\mathbb{R}\) for kernel classification
					\end{itemize}
	\end{enumerate}
such that :
	
	\[
		\begin{array}{l}
		t_{optimal} = \\
		= inf\{t | Feas(t)\mbox{ is feasible}\} \\
		= sup\{t | Feas(t)\mbox{ is not feasible}\} \\
		\end{array}
	\]
	
	which gives the following algorithm for solving:
	
	\begin{algorithm}
	\caption{Binary Search Algorithm Used by \textit{PVM}}
	\label{algBinarySearch}
	\begin{center}
	\begin{algorithmic}
	
		\STATE \(t_{right} \gets 0\)
		\FOR{i = 0; i < \(i_{max}\); i++}
			\STATE \(t_{left}\gets t_{right}\)
			\STATE \(t_{right}\gets 2^i\)
			\IF{\(Feas(t_{right})\) is feasible}
				\STATE break
			\ENDIF
		\ENDFOR
		
		\IF{i == \(i_{max}\)}
			\RETURN sets{\_}are{\_}identical
		\ENDIF
		
		\WHILE{\(t_{right} - t_{left} > \epsilon\)}
			\STATE \(t_{center}\gets \frac{1}{2} (t_{left} + t_{right})\)
			\IF{\(Feast(t_{center})\) is feasible}
				\STATE \(t_{right}\gets t_{center}\)				
			\ELSE
				\STATE \(t_{left}\gets t_{center}\)
			\ENDIF			
		\ENDWHILE
		
		\STATE \(t_{optimal} = \frac{1}{2} (t_{left} + t_{right})\)	
	\end{algorithmic}
	\end{center}
	\end{algorithm}
\clearpage	
	
	Algorithm \ref{algBinarySearch} gives the value of \(t_{optimal}\) and the last successful resolution of \(Feas(t)\) gives the values for the sought variables.
	
	
\section{Solving the Feasibility Problem}

	As can be seen in \ref{pvm_feas}, the classification problem requires the solution to a set of linear feasibility problems (LFP), \(Feas(t)\).
	
	An LFP may be expressed as:	
	\[
		Ax \geq 0
	\]	
	where \(A\in M_{m\times n}(\mathbb{R})\) is the coefficients matrix and \(x\in \mathbb{R} ^n\) is the vector of unknowns.	
	\(A\) may also be viewed as \(A = \{a_i \in \mathbb{R}^n | i\in\overline{1..m}\}\). The LFP may also be expressed as :	
	\[
		<a_i, x> \geq 0 \mbox{ , where } i\in\overline{1..m}
	\]
	
	For such a matrix, we will denote by \(B = \{B_i|i\in\overline{1..k}\}\) a block cover for \(A\) :
	
	\begin{itemize}
		\item \(B_i = \{a_{i_0}, a_{i_1}, ... , a_{i_l} | a_{i_j}\in A\}\)
		\item \(\cup_{i=\overline{1..k}} B_i = A\)
	\end{itemize}
	
	Denote by \(Idx_i = \{i_0, i_1, ..., i_l\}\) the set of indices associated with block \(B_i\). 

	For a LFP with a block cover, \(B\), of the system matrix, we will denote by \(LFP_l\) the feasibility problem reduced to the \(l\)-th block:
	
	\[
		<a_i, x> \geq 0 \mbox{ , where } i\in Idx_l
	\]
	
	Denote by \(x_l\) the current solution to \(LFP_l\). 

	Pending the definition of the various procedures used, the pseudocode for solving the LFP is:
	
	\begin{algorithm}
	\caption{\textit{PVMFP}}
	\label{algPVMFP}
	\begin{center}
	\begin{algorithmic}
	
	\STATE \(iter\gets 0, x\gets 0\)
	\STATE \(flag{\_}cyclic \gets 0, flag{\_}infeasible \gets 0, flag{\_}solved \gets 0\)
	\STATE \(feasibility{\_}gap \gets \inf\)
	
	\WHILE{\(iter < iter_{max}\)}
		\STATE \(flag{\_}solved \gets 1\)
		\STATE \(feasibility{\_}gap \gets 0\)
		\FOR{\(l = 0; l < k; l++\)}
			\STATE PartiallySolveBlock(\(LFP_l\))
			\IF{\(LFP_l\) is infeasible}
				\STATE \(flag{\_}infeasible \gets 1\)
				\STATE break
			\ENDIF
			
			\IF{!(\(LFP_l\) is solved)}
				\STATE \(flag{\_}solved \gets 0\)
			\ELSE
				\STATE \(feasibility{\_}gap += LFP_l.feasibility{\_}gap\)
			\ENDIF								
		\ENDFOR
		
		\STATE \(x\gets ComponentAverage(x_0, x_1, ..., x_{k-1})\)
		
		\IF {flag{\_}infeasible || flag{\_}solved}
			\STATE break
		\ENDIF
		
		\IF{SolutionCycles()}
			\STATE \(flag{\_}cyclic \gets 1 \)
			\STATE break
		\ENDIF
		
		\STATE \(iter++\)
		
	\ENDWHILE
	
	
	\IF{\(flag{\_}infeasible || flag{\_}cyclic || iter == iter_{max}\)}
		\RETURN \(PVMFP\) is infeasible
	\ELSE
		\RETURN \(PVMFP\) is feasible
	\ENDIF
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}		
	
\clearpage		
	
	What is left is to define the various procedures used in the above pseudocode.
	
	
	\subsection{PartiallySolveBlock}
	
	
	\subsubsection{The Perceptron Method}
		This procedure is to be used on a single block of equations, \(B_l\). For ease of notation, we will refer to the equations 
in the block as \(A = \{a_i\in\mathbb{R} ^n | i\in\overline{1..m}\}\). The set of vectors will only refer to the set of vectors current in the block.

	The init procedure for the block will be : 
	%mcu: cred ca aici ti-a aranjat latex algoritmii in ordinea inversa
	
	\begin{algorithm}
	\caption{\textit{Block Init}}
	\label{algBlockInit}
	\begin{center}
	\begin{algorithmic}
	
		\STATE Input A
		\STATE rescale{\_}count = 0, \(R = I_n\), \(R^{-1} = I_n\)	
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}			
	
	The PartillySolveBlock procedure:
	
	
	\begin{algorithm}
	\caption{\textit{PartiallySolveBlock}}
	\label{algPartialSolve}
	\begin{center}
	\begin{algorithmic}
	
	\STATE \textbf{INPUT :} \(x\in \mathbb{R}\) 	
	\STATE \textbf{OUTPUT :} \(x\in \mathbb{R}\), flag{\_}solve, flag{\_}infeasible, \(dist_{max}\in\mathbb{R}_+\)
	
	\STATE iter{\_}count = 0, \(\sigma = \frac{1}{32n}\)
	\STATE \(x_r = R^{-1} x\), \(A_r = A\cdot R\)
	
	\WHILE{\(iter{\_}count < iter_{max}\)}
	\STATE
	\STATE \textit{Perceptron Phase}	
	\FOR {i = 0; i < \(\frac{1}{\sigma^2}; i++\)}
		\FOR {j = 0; j < \(A_r\).count; j++}
			\IF {\(<A_r(j), x_r> \leq 0\)}
				\STATE \(x_r += \overline{A_r(j)}\)
				\STATE break
			\ENDIF
		\ENDFOR			
	\ENDFOR
	
	\IF{\(A_r \cdot x_r \geq 0\)}
		\RETURN \(R\cdot x_r\)
	\ENDIF
	\STATE
	\STATE \textit{Perceptron Improvement}
	
	\LOOP	
	\STATE Choose \(x_{imp}\in\mathbb{S}^n\) uniformly random
	
	\FOR {i = 0; i < \(\frac{\ln n}{\sigma^2}; i++\)}
		\FOR {j = 0; j < \(A_r\).count; j++}
			\IF {\(<A_r(j), x_r> < -\sigma\)} %mcu: cine este acest '-\sigma' ? -> este constanta initializata la inceputul algoritmului cu \frac{1}{32n}
				\STATE \(x_{imp} -= (\overline{A_r(j)}\cdot \overline{x_{imp}})x_{imp} \ \)
				\STATE break
			\ENDIF
		\ENDFOR
	\ENDFOR
	
	\STATE \(flag{\_}repeat \gets 0\)
	
	\FOR{j = 0; j < \(A_r\).count; j++}
			\IF {\(<A_r(j), x_r> < -\sigma\)}
				\STATE \(flag{\_}repeat \gets 1\)
				\STATE break
			\ENDIF		
	\ENDFOR
	
	\IF{!flag{\_}repeat}
		\STATE break
	\ENDIF	
		
	\ENDLOOP
	
	\STATE
	\STATE \textit{Scaling}	
	
	\STATE \(R_{iter{\_}count} = I_n + \overline{x_{imp}} \cdot \overline{x_{imp}}^T \)
	\STATE \(R^{-1}_{iter{\_}count} = I_n - \frac{1}{2} \overline{x_{imp}} \cdot \overline{x_{imp}}^T \)
	
	\STATE \(R \gets R \cdot R_{iter{\_}count}\)
	\STATE \(R^{-1} \gets R^{-1}_{iter{\_}count}\cdot R^{-1}\)
	\STATE \(A_r \gets A_r \cdot R_{iter{\_}count}\)
	\STATE \(x_r \gets R^{-1}_{iter{\_}count} \cdot x_r\)
	
	
	\STATE \(iter{\_}count++\)
	
	\ENDWHILE
	\end{algorithmic}
	\end{center}
	\end{algorithm}		
	
\clearpage	

	\subsubsection{The Averaged Update Method}
	
	Unlike the previous perceptron method, this method only requires typical solution updates, without any rescaling of the constraints.
	
	In order to compute a solution update, at one iteration, we will first compute the updates suggested by every constraint that is not satisfied. The final update will be a convex combination of these individually suggested updates. 
	
	Consider that \(A\) is the set of constraints corresponding to the block, with every constraint rescaled to norm 1 : \(A = \{a_i\in \mathbb{R}^n | i = \overline{0..m-1}\}\).
	
	Consider \(a\in A\) for which the associated constraint is not satisfied, e.g. if the asociated constraint is \(<a,x> \geq 0\), then it not being satisfied would mean \(<a, x> < 0\); if the associated constraint is \(<a, x> > 0\), then it not being satisfied would mean \(<a, x> \leq 0\). The update associated with such a constraint would then be :
	
		\begin{itemize}
			\item \(u(a) = -<a, x> a\) for a \(\geq\) type constraint
			\item \(u(a) = -(<a, x> - \xi) a\), where \(\xi > 0\) is a positive constant 
		\end{itemize}
	
	
	Denote by \(S(a) = ||u(a)||\) the score associated to this suggested update. Denote by \(n_1, n_2,..., n_k\) the indexes of the constraints that are not satisfied. Then we will have:
	
	\begin{itemize}
		\item \(a_{n_1}, a_{n_2},\ldots, a_{n_k}\) the constraints that are not satisfied
		\item \(u(a_{n_1}), u(a_{n_2}),\ldots, u(a_{n_k})\) the updates suggested by every constraint that is not satisfied
		\item \(S(a_{n_1}), S(a_{n_2}),\ldots, S(a_{n_k})\) the scores associated to each such constraint
	\end{itemize}
	
	We will compute the update of the solution at the current iteration, \(U_{iter}\), as a convex combination of the suggested updates. This means that we will be looking for \(p_1, p_2, \ldots, p_k \in [0, 1]\) such that \(\sum{p_i} = 1\), implying the computation of the update as :	
	\[
		U_{iter} = \lambda_{iter} \sum{p_i \cdot u(a_{n_i})}
	\] 
	where \(\lambda_{iter}\in [\delta, 2-\delta]\), \(\delta > 0\), is a relaxation parameter.
	
	We will denote by \(max_c\) the maximum number of constraints that will contribute at one time to the update. Let \(\alpha \in [0, \infty]\). We will compute the weights \(p_i\) as:
	
	\begin{itemize}
		\item Sort \(a_{n_1}, a_{n_2},\ldots, a_{n_k}\) decending, according to their scores.
		\item Compute \(S = \sum^{max_c}_{i = 1} {(S_{a_{n_i}})^\alpha}\)
		\item For \(i > max_c\), \(p_i = 0\)
		\item For \(i \in \overline{1..max_c}\), \(p_i = \frac{(S_{a_{n_i}})^\alpha}{S}\)
	\end{itemize}
	
	Note that for \(\alpha \rightarrow \infty\), \(p_0 = 1\) and the rest will be 0.
	
	The update procedure will be summed up in algorithm \ref{algPartialSolveAvg}.
	
	\begin{algorithm}
	\caption{\textit{PartiallySolveBlockAveraged}}
	\label{algPartialSolveAvg}
	\begin{center}
	\begin{algorithmic}
	
	\STATE Input : \(x\in \mathbb{R}^n\), \(max\_iters\), \(A\in M_{m\times n}(\mathbb{R})\)
	\STATE \(iter\_count \gets 0\)
	
	\WHILE{\(iter\_count < max\_iters\)}
	
		\STATE UnSatConstraints.Clear()
		
		\FOR{i = 0; i < A.count; i++}
			\IF{(\(a_i\).type == GreaterEqual \(\cap\) \(<a_i, x> < 0\))}
				\STATE UnSatConstraints.PushBack(\(a_i, -<a_i, x> a_i\))
			\ELSE
				\IF {(\(a_i\).type == Greater \(\cap\) \(<a_i, x> <= 0\))}
					\STATE UnSatConstraints.PushBack(\(a_i, -(<a_i, x> - \xi)a_i\))
				\ENDIF
			\ENDIF
		\ENDFOR
		
		\IF{!UnSatConstraints.count}
			\STATE break
		\ENDIF 
		
		\STATE UnSatConstraints.Sort()
		
		\FOR{(i = 0; i < UnSatConstraints.count; i++)}
			\STATE \(p_i \gets 0\)
		\ENDFOR
		
		\STATE \(S \gets 0\)
		
		\FOR{(i = 0; i < \(max_c\) \(\cap\) i < UnSatConstraints.count; i++)}
			\STATE S += \(S_i ^\alpha\)
		\ENDFOR
		
		\FOR{(i = 0; i < \(max_c\) \(\cap\) i < UnSatConstraints.count; i++)}
			\STATE \(p_i = S_i ^\alpha\) / S
		\ENDFOR
		
		\STATE \(U \gets 0_n\)
		
		\FOR{(i = 0; i < \(max_c\) \(\cap\) i < UnSatConstraints.count; i++)}
			\STATE U += \(p_i \cdot u_i\)
		\ENDFOR
		
		\STATE x += U
	
		\STATE \(iter\_count ++\)
	\ENDWHILE

	\end{algorithmic}
	\end{center}
	\end{algorithm}			
	
	\clearpage
		
	\subsection{ComponentAverage}
	
		Let \(x_0, ..., x_k\) be the partial solutions outputed by the PartiallySolveBlock procedure for every block, \(B_0, ..., B_k\). 

		If \(x_l = (x^0_l, ..., x^{n-1}_l)\), we will denote by \(SI(i) = \{idx_t| idx_t \in\overline{0..k}\}\) the set of indices for which the \(i\)-th component of the 
point in \(\mathbb{R} ^n\) has at least an equation in block \(B_{idx_t}\) that has a nonzero coefficient corresponding to that \(i\)-th component. \(SI(i)\) tracks all the 
blocks that influence component \(i\).

	The input/output for the ComponentAverage procedure is :
	
	\begin{itemize}
		\item \textbf{Input} The partial solutions \(x_0, ..., x_k \in \mathbb{R} ^n\)
		\item \textbf{Output} The current overall iterate \(x = (x^0, x^1,..., x^{n-1})\in \mathbb{R} ^n\)
	\end{itemize}
	
	The ComponentAverage procedure is:
	
	\begin{enumerate}
		\item \(x^i = \frac{1}{|SI(i)|} \cdot \sum_{j\in SI(i)} {x^i_j}\)
		\item Output \(x = (x^0, x^1, ..., x^{n-1})\)
	\end{enumerate}		
	
	
	Simply put, the ComponentAverage computes the average of a coordonate only over the blocks that influence that coordonate.
		
	\subsection{SolutionCycles}
	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIBLIOGRAPHY AND OTHER LISTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% A small distance to the other stuff in the table of contents (toc)
\addtocontents{toc}{\protect\vspace*{\baselineskip}}

%% The Bibliography
%% ==> You need a file 'literature.bib' for this.
%% ==> You need to run BibTeX for this (Project | Properties... | Uses BibTeX)
%\addcontentsline{toc}{chapter}{Bibliography} %'Bibliography' into toc
%\nocite{*} %Even non-cited BibTeX-Entries will be shown.
%\bibliographystyle{alpha} %Style of Bibliography: plain / apalike / amsalpha / ...
%\bibliography{literature} %You need a file 'literature.bib' for this.

%% The List of Figures
\clearpage
\addcontentsline{toc}{chapter}{List of Figures}
\listoffigures

%% The List of Tables
\clearpage
\addcontentsline{toc}{chapter}{List of Tables}
\listoftables


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% APPENDICES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
%% ==> Write your text here or include other files.

%\input{FileName} %You need a file 'FileName.tex' for this.


\end{document}

