%% Based on a TeXnicCenter-Template by Tino Weinkauf.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% HEADER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,twoside,10pt]{report}
% Alternative Options:
%	Paper Size: a4paper / a5paper / b5paper / letterpaper / legalpaper / executivepaper
% Duplex: oneside / twoside
% Base Font Size: 10pt / 11pt / 12pt


%% Language %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[USenglish]{babel} %francais, polish, spanish, ...
\usepackage[T1]{fontenc}
\usepackage[ansinew]{inputenc}

\usepackage{lmodern} %Type1-font for non-english texts and characters


%% Packages for Graphics & Figures %%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx} %%For loading graphic files
%\usepackage{subfig} %%Subfigures inside a figure
%\usepackage{tikz} %%Generate vector graphics from within LaTeX

%% Please note:
%% Images can be included using \includegraphics{filename}
%% resp. using the dialog in the Insert menu.
%% 
%% The mode "LaTeX => PDF" allows the following formats:
%%   .jpg  .png  .pdf  .mps
%% 
%% The modes "LaTeX => DVI", "LaTeX => PS" und "LaTeX => PS => PDF"
%% allow the following formats:
%%   .eps  .ps  .bmp  .pict  .pntg


%% Math Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}

%% Line Spacing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{setspace}
%\singlespacing        %% 1-spacing (default)
%\onehalfspacing       %% 1,5-spacing
%\doublespacing        %% 2-spacing


%% Other Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{a4wide} %%Smaller margins = more text per page.
%\usepackage{fancyhdr} %%Fancy headings
%\usepackage{longtable} %%For tables, that exceed one page


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Remarks
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% TODO:
% 1. Edit the used packages and their options (see above).
% 2. If you want, add a BibTeX-File to the project
%    (e.g., 'literature.bib').
% 3. Happy TeXing!
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Options / Modifications
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\input{options} %You need a file 'options.tex' for this
%% ==> TeXnicCenter supplies some possible option files
%% ==> with its templates (File | New from Template...).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% DOCUMENT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\pagestyle{empty} %No headings for the first pages.


%% Title Page %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ==> Write your text here or include other files.

%% The simple version:
\title{PVM Algorithm Pseudocode}
\author{Andrei Sucil\u a}
%\date{} %%If commented, the current date is used.
\maketitle

%% The nice version:
%\input{titlepage} %%You need a file 'titlepage.tex' for this.
%% ==> TeXnicCenter supplies a possible titlepage file
%% ==> with its templates (File | New from Template...).


%% Inhaltsverzeichnis %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents %Table of contents
\cleardoublepage %The first chapter should start on an odd page.

\pagestyle{plain} %Now display headings: headings / fancy / ...



%% Chapters %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ==> Write your text here or include other files.

%\input{intro} %You need a file 'intro.tex' for this.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ==> Some hints are following:

\chapter{Introduction}\label{intro}


\section{List of Adnotations}\label{adnote_list}

	\begin{itemize}
		\item \(\mathbb{R}^n\) the \(n\)-dimensional real space
		\item \(H\) a Hilbert space
		\item \(<x_0, x_1>\) the scalar product for \(x_0, x_1\in H\)
		\item \(x_i \in \mathbb{R}^n\) a general point in \(n\) dimensions with \(y_i\in \{\pm 1\} \}\) its label
		\item \(\overline{a}\), where \(a\in\mathbb{R}^n\), is the unit vector corresponding to \(a\)
		\item \(<x_0, x_1>\), where \(x_0, x_1 \in \mathbb{R}^n\), is the scalar product
		\item \(S = \{(x_i, y_i) | x_i \in \mathbb{R}^n, y_i\in \{\pm 1\} \}, i\in \overline{1..m}\) - the initial training set
		\item \(S_+ = \{x_i\in \mathbb{R}^n | y_i = 1 \} \), \(S_- = \{x_i\in \mathbb{R}^n | y_i = -1 \} \) the positively labeled subset of 
							training points and the negatively labeled subset of training points
		\item \(b\in \mathbb{R}\) the bias of the separating hyperplane
		\item \(\omega \in \mathbb{R}^n\), if no kernel is used, this is the normal vector to the separating hyperplane; the equation for the separating 
							hyperplane is thus expressed as :
							\begin{itemize}
								\item \(<\omega, x> + b \geq 0\) for the positive side of the separation
								\item \(<\omega, x> + b \leq 0\) for the negative side of the separation
							\end{itemize}
		\item \(\alpha_i \in \mathbb{R}\), where \(i\in\overline{1..m}\) the kernel members coefficients for when a kernel is used; the equation for the separating 
							hyperplane is thus expressed as :
							\begin{itemize}
								\item \(\sum^m_{i=1}{\alpha_i K(x_i, x)} + b \geq 0\) for the positive side of the separation
								\item \(\sum^m_{i=1}{\alpha_i K(x_i, x)} + b \leq 0\) for the negative side of the separation
								%mcu: aici am pus eu sum^m in loc de sum^n -- sunt m items conform cu ce ai zis tu mai sus
								%ai dreptate aici
							\end{itemize}
							
		\item	\(A\in M_{m\times n}(\mathbb{R})\) an \(m\times n\) (\(m\)lines, \(n\) columns) matrix with real coefficients
	\end{itemize}
	
	
	
\chapter{Pseudocodes}
	
\section{PVM Problem}\label{pvm_feas}

\subsection{Initial Formulation}

	The formulation for the PVM Feasibility Problem(PVMFP), parametrized by \( t \in \mathbb{R}_+ \), will be:
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				\frac{1}{|S_+|} \sum_{x\in S_+} {(<w,x> + b)} = {E_+}\\
				\frac{1}{|S_-|} \sum_{x\in S_-} {(<w,x> + b)} = -{E_-}\\
				
				|(<w,x_i> + b) - E_+| \leq \sigma_+^i, x_i\in S_+\\
				|(<w,x_i> + b) + E_-| \leq \sigma_-^i, x_i\in S_-\\
				
				\frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {\sigma_+^i} = {\sigma_+} \\ %mcu: aici nu ai zis nicaieri cine este \sigma_+ -> este o necunoscuta in sistem, introdusa prin egalitatea asta
				\frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {\sigma_-^i} = {\sigma_-} \\ %corespunde deviatiei medii a distantelor, dar oricum va fi eliminata in ver de mai jos a sistemului
				
				\sigma_+ \leq t \cdot E_+\\
				\sigma_- \leq t \cdot E_-\\
				E_+ > 0\\
				E_- > 0\\
		\end{array}
		\right. \label{feas_0}
	\]
	
	The kernel formulation for the PVMFP, parametrized by \( t \in \mathbb{R}_+ \), will be:
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				b + \sum_{x_i\in S} {\alpha_i \cdot \frac{1}{|S_+|}\sum_{x_j\in S_+} {K(x_i, x_j)}} = E_+\\
				b + \sum_{x_i\in S} {\alpha_i \cdot \frac{1}{|S_-|}\sum_{x_j\in S_-} {K(x_i, x_j)}} = E_-\\
				
				|\sum_{x_i\in S} {\alpha_i K(x_i, x_j)} + b - E_+| \leq \sigma^j_+ \mbox{ , for each } x_j \in S_+\\
				|\sum_{x_i\in S} {\alpha_i K(x_i, x_j)} + b + E_-| \leq \sigma^j_- \mbox{ , for each } x_j \in S_-\\
					
				\frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {\sigma_+^i} = {\sigma_+} \\
				\frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {\sigma_-^i} = {\sigma_-} \\
				
				\sigma_+ \leq t \cdot E_+\\
				\sigma_- \leq t \cdot E_-\\
				E_+ > 0\\
				E_- > 0\\
		\end{array}
		\right. \label{feas_1}
	\]	
	
	In order to rewrite the above, let us denote by:
	
	\[
		\begin{array}{l}
			K^i_+ = \frac{1}{|S_+|}\sum_{x_j \in S_+} {K(x_i, x_j)} \\
			K^i_- = \frac{1}{|S_-|}\sum_{x_j \in S_-} {K(x_i, x_j)} \\
		\end{array}
	\]
	
	By integrating the equalities, the system then becomes:
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				|\sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)}| \leq \sigma^j_+ \mbox{ , for each } x_j \in S_+\\
				|\sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)}| \leq \sigma^j_+ \mbox{ , for each } x_j \in S_+\\								
				
				\frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {\sigma_+^i} \leq t \cdot (b + \sum_{x_i\in S}{\alpha_i K^i_+})\\
				\frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {\sigma_-^i} \leq t \cdot (-b - \sum_{x_i\in S}{\alpha_i K^i_-})\\
				b + \sum_{x_i\in S}{\alpha_i K^i_+} > 0\\
				- b - \sum_{x_i\in S}{\alpha_i K^i_-} > 0\\
		\end{array}
		\right. \label{feas_2}
	\]	
	
	We can rewrite this in a standardized form as:
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				\sigma^j_+ - (\sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)}) \geq 0 \mbox{ , for each } x_j \in S_+\\
				\sigma^j_+ + \sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)} \geq 0 \mbox{ , for each } x_j \in S_+\\
				
				\sigma^j_- - (\sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_-)}) \geq 0 \mbox{ , for each } x_j \in S_-\\
				\sigma^j_- + \sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_-)} \geq 0 \mbox{ , for each } x_j \in S_-\\									
				
				t \cdot (b + \sum_{x_i\in S}{\alpha_i K^i_+}) - \frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {\sigma_+^i} \geq 0\\
				t \cdot (- b - \sum_{x_i\in S}{\alpha_i K^i_-}) - \frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {\sigma_-^i} \geq 0\\
				b + \sum_{x_i\in S}{\alpha_i K^i_+} > 0\\
				- b - \sum_{x_i\in S}{\alpha_i K^i_-} > 0\\
				\epsilon \geq 0\\
		\end{array}
		\right. \label{feas_3}
	\]		
	
	The system \ref{feas_3} is the system that will be passed to the feasibility solver.
	
\subsection{Adding Weights}	
	
	It may be necessary that we add weights to points. One such example would be when, prior to training, a clustering on the initial data 
is undertaken in order to minimize the number of training points, by replacing them with the centroids of the clusters. In this case, we would want 
a centroid to corectly represent its cluster by contributing to the average and average deviation proportionally to the number of elements represented in the cluster.

	The following notations shall be used: 
	
	\begin{itemize}
	 \item \(p_i \in [1, \infty)\) shall denote the weight of \(x_i\). 
	 \item \(|S_+| =\sum_{i : x_i\in S_+}{p_i}\) the sum of weights coresponding to the positively labeled points
	 \item \(|S_-| =\sum_{i : x_i\in S_-}{p_i}\) the sum of weights coresponding to the negatively labeled points
	 \item \(K^i_+ = \frac{1}{|S+|} \sum_{x_j\in S_+}{p_j K(x_i, x_j)}\)
	 \item \(K^i_- = \frac{1}{|S-|} \sum_{x_j\in S_-}{p_j K(x_i, x_j)}\)
	\end{itemize}
	
	Note that, if \(S_+\) has at least 2 members, then \(|S_+| > 1\) and thus \(|S_+| - 1 > 0\). Similarly, \(|S_-| - 1 > 0\).
	
	The feasibility problem coresponding to \(t\in\mathbb{R}_+\) becomes :
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				\frac{1}{|S_+|} \sum_{x_i\in S_+} {p_i(<w,x_i> + b)} = {E_+}\\
				\frac{1}{|S_-|} \sum_{x_i\in S_-} {p_i(<w,x_i> + b)} = -{E_-}\\
				
				|(<w,x_i> + b) - E_+| \leq \sigma_+^i, x_i\in S_+\\
				|(<w,x_i> + b) + E_-| \leq \sigma_-^i, x_i\in S_-\\
				
				\frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {p_i \sigma_+^i} = {\sigma_+} \\ %mcu: aici nu ai zis nicaieri cine este \sigma_+ -> este o necunoscuta in sistem, introdusa prin egalitatea asta
				\frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {p_i \sigma_-^i} = {\sigma_-} \\ %corespunde deviatiei medii a distantelor, dar oricum va fi eliminata in ver de mai jos a sistemului
				
				\sigma_+ \leq t \cdot E_+\\
				\sigma_- \leq t \cdot E_-\\
				E_+ > 0\\
				E_- > 0\\
		\end{array}
		\right. \label{feas__weight_0}
	\]
	
	Introducing the kernel, the system becomes : 
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				b + \sum_{x_i\in S} {\alpha_i \cdot [\frac{1}{|S_+|}\sum_{x_j\in S_+} {p_j K(x_i, x_j)}]} = E_+\\
				b + \sum_{x_i\in S} {\alpha_i \cdot [\frac{1}{|S_-|}\sum_{x_j\in S_-} {p_j K(x_i, x_j)}]} = -E_-\\
				
				|\sum_{x_i\in S} {\alpha_i K(x_i, x_j)} + b - E_+| \leq \sigma^j_+ \mbox{ , for each } x_j \in S_+\\
				|\sum_{x_i\in S} {\alpha_i K(x_i, x_j)} + b + E_-| \leq \sigma^j_- \mbox{ , for each } x_j \in S_-\\
					
				\frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {p_i \sigma_+^i} = {\sigma_+} \\
				\frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {p_i \sigma_-^i} = {\sigma_-} \\
				
				\sigma_+ \leq t \cdot E_+\\
				\sigma_- \leq t \cdot E_-\\
				E_+ > 0\\
				E_- > 0\\
		\end{array}
		\right. \label{feas_weight_1}
	\]	

	By using the agreed upon notation, the system becomes:
	
	\[
		Feas(t) = \left\{
		\begin{array}{l}			
				b + \sum_{x_i\in S} {\alpha_i \cdot K^i_+} = E_+\\
				b + \sum_{x_i\in S} {\alpha_i \cdot K^i_-} = -E_-\\
				
				|\sum_{x_i\in S} {\alpha_i K(x_i, x_j)} + b - E_+| \leq \sigma^j_+ \mbox{ , for each } x_j \in S_+\\
				|\sum_{x_i\in S} {\alpha_i K(x_i, x_j)} + b + E_-| \leq \sigma^j_- \mbox{ , for each } x_j \in S_-\\
					
				\frac{1}{|S_+| - 1} \sum_{x_i\in S_+} {p_i \sigma_+^i} = {\sigma_+} \\
				\frac{1}{|S_-| - 1} \sum_{x_i\in S_-} {p_i \sigma_-^i} = {\sigma_-} \\
				
				\sigma_+ \leq t \cdot E_+\\
				\sigma_- \leq t \cdot E_-\\
				E_+ > 0\\
				E_- > 0\\
		\end{array}
		\right. \label{feas_weight_2}
	\]	
	
	
	Replacing the equalities in the other constraints, the system becomes:
 

	\[
		Feas(t) = \left\{
		\begin{array}{l}			
							
				|\sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)}| \leq \sigma^j_+ \mbox{ , for each } x_j \in S_+\\
				|\sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_-)}| \leq \sigma^j_- \mbox{ , for each } x_j \in S_-\\
									
				t(|S_+| - 1)\sum_{i : x_i \in S}{\alpha_i \cdot K^i_+} + t(|S_+| - 1)b - \sum_{x_i\in S_+}{p_i \sigma^i_+} \geq 0\\
				t(|S_-| - 1)\sum_{i : x_i \in S}{\alpha_i \cdot (-K^i_-)} - t(|S_-| - 1)b - \sum_{x_i\in S_-}{p_i \sigma^i_-} \geq 0\\								
				
				b + \sum_{x_i\in S} {\alpha_i \cdot K^i_+} > 0\\
				-b + \sum_{x_i\in S} {\alpha_i \cdot (-K^i_-)} > 0\\				
		\end{array}
		\right. \label{feas_weight_3}
	\]	
	
	This last form, \ref{feas_weight_3}, is the system that is to be solved by the feasibility solver, with unknowns \(\alpha_i, \sigma^i_+, \sigma^i_-\) and \(b\). 
	
	
\subsection {Feasibility System Formulation Pseudocode}	

	The pseudocode for formulating the feasibility system shall be detailed in this subsection.
	
	
	First, the computation of \(K^i_+\) 
	
	\begin{algorithm}
	
	\caption{Computation of \(K^i_+\)}
	\label{comp_ki_plus}		
	
	\begin{center}
	\begin{algorithmic}	
	
		\STATE Input \(x_i\)
		\STATE \(ReturnValue \gets 0\)
		
		\FOR {(\(x_j = S_+\).begin; \(x_j < S_+\).end; \(x_j++\))}
			\STATE ReturnValue += \(p_j \cdot K(x_i, x_j)\)
		
		\ENDFOR
		
		\STATE ReturnValue /= \(|S_+|\)
		
		\STATE return ReturnValue
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}	

	and \(K^i_-\):
	
	
	\begin{algorithm}
	
	\caption{Computation of \(K^i_-\)}
	\label{comp_ki_minus}		
	
	\begin{center}
	\begin{algorithmic}	
	
		\STATE Input \(x_i\)
		\STATE \(ReturnValue \gets 0\)
		
		\FOR {(\(x_j = S_-\).begin; \(x_j < S_-\).end; \(x_j++\))}
			\STATE ReturnValue += \(p_j \cdot K(x_i, x_j)\)
		
		\ENDFOR
		
		\STATE ReturnValue /= \(|S_-|\)
		
		\STATE return ReturnValue
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}	


	For the formulation of system \ref{feas_weight_3}, the following procedures will be considered as already defined:
	
	\begin{itemize}
		\item Indexing of all the unknown variables. The index of a variable, \(var\), will be given by \(idx(var)\) (e.g. \(idx(\alpha_i)\)). Indexes will start from 0 and end with the number of unknowns.
		\item The init procedure for a new constraint, \(cons\), will be given by \(cons = NewConstraint()\). 
		\item The \(clear\) procedure for a constraint will clear it of its current contents. 
		\item A constraint will also have the \(set\) function which, for a given index, sets the value of the coefficient. e.g. \(cons.set(idx(\alpha_i), val)\) sets the coefficient for \(\alpha_i\) in \(cons\) to value \(val\). For the indexes for which the \(set\) function has not been called, the value will implicitly be 0. 
		\item A constraint will also have the \(get\) function which, for a given index, returns the value for the coefficient of that index, e.g. \(cons.get(idx(\alpha_i))\) returns the value of the coefficient set for \(\alpha_i\), or 0 in case that that coefficient has not yet been set.
		\item Denote by \(FeasSys\) the currently constructed feasibility system. Denote by \(PushBack\) the procedure of adding a constraint of a type, with a right hand value, to the system by copying it e.g. \(FeasSys.PushBack(cons, GreaterEqual, RHV)\).
	\end{itemize}
	
	The algorithm for constructing the feasibility system \ref{feas_weight_3} is split in three parts.
	
	The first part introduces the deviations from the averages:	
	
	\begin{algorithm}
	
	\caption{Feasibility System Construction Part 1}
	\label{feassystemconstruction1}		
	
	\begin{center}
	\begin{algorithmic}	
	
	\STATE Input : \(t\in \mathbb{R}_+, S_+, S_-\)
	
	%aici se adauga constrangerile care definesc deviatiile de la media pozitiva a pct cu label pozitiv:
	
	\STATE cons = NewConstraint()
	\FOR {\(x_j = S_+.begin\); \(x_j < S_+.end\); \(x_j ++\)}
		%adaugam prima constrangere pentru definirea inegalitatii cu modul : sigma^j_+ >= Sum(alpha_i * (K(x_i, x_j) - K^i_+))
		%forma echivalenta: sigma^j_+ + Sum(alpha_i * (K^i_+ - K(x_i, x_j))) >= 0
		\STATE cons.Clear()
		
		\FOR{(i = 0; i < S.count; i++)}
			\STATE \(cons.set(idx(\alpha_i), K^i_+ - K(x_i, x_j))\)
		\ENDFOR
		
		\STATE \(const.set(idx(\sigma^j_+), 1.0)\)
		\STATE \(FeasSys.PushBack(cons, GreaterEqual, 0)\)
		
		%adaugam prima constrangere pentru definirea inegalitatii cu modul : sigma^j_+ >= -Sum(alpha_i * (K(x_i, x_j) - K^i_+))
		%forma echivalenta: sigma^j_+ + Sum(alpha_i * (K(x_i, x_j) - K^i_+)) >= 0
		%observa ca forma aceasta are doar semnele schimbate pt coef alpha_i			
		\FOR{(i = 0; i < S.count; i++)}
			\STATE \(cons.set(idx(\alpha_i), - cons.get(idx(\alpha_i)))\)
		\ENDFOR
				
		\STATE \(FeasSys.PushBack(cons, GreaterEqual, 0)\)		
		
	\ENDFOR
	
	%aici se adauga constrangerile care definesc deviatiile de la media negativa a pct cu label negativ:
	\FOR {\(x_j = S_-.begin\); \(x_j < S_-.end\); \(x_j ++\)}
		%adaugam prima constrangere pentru definirea inegalitatii cu modul : sigma^j_- >= Sum(alpha_i * (K(x_i, x_j) - K^i_-))
		%forma echivalenta: sigma^j_- + Sum(alpha_i * (K^i_- - K(x_i, x_j))) >= 0
		\STATE cons.Clear()
		
		\FOR{(i = 0; i < S.count; i++)}
			\STATE \(cons.set(idx(\alpha_i), K^i_- - K(x_i, x_j))\)
		\ENDFOR
		
		\STATE \(cons.set(idx(\sigma^j_-), 1.0)\)
		\STATE \(FeasSys.PushBack(cons, GreaterEqual, 0)\)
		
		%adaugam prima constrangere pentru definirea inegalitatii cu modul : sigma^j_- >= -Sum(alpha_i * (K(x_i, x_j) - K^i_-))
		%forma echivalenta: sigma^j_- + Sum(alpha_i * (K(x_i, x_j) - K^i_-)) >= 0
		%observa ca forma aceasta are doar semnele schimbate pt coef alpha_i			
		\FOR{(i = 0; i < S.count; i++)}
			\STATE \(cons.set(idx(\alpha_i), - cons.get(idx(\alpha_i)))\)
		\ENDFOR
				
		\STATE \(FeasSys.PushBack(cons, GreaterEqual, 0)\)		
	\ENDFOR
		
	\STATE delete cons
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}	
	
	
	
	The second part imposes the inequality between the average deviations and the averages:
	
	\begin{algorithm}
	
	\caption{Feasibility System Construction Part 2}
	\label{feassystemconstruction2}		
	
	\begin{center}
	\begin{algorithmic}	
	
	\STATE cons = NewConstraint()
	
	%adaug constrangerea pentru media pozitiva si deviatia medie pozitiva
	%  t(|S_+| - 1)\sum{\alpha_i K^i_+} + t(|S_+| - 1)b - \sum{\sigma^i_+ p_i}
	\FOR{(i = 0; i < S.count; i++)}
		\STATE \(cons.set(idx(\alpha_i), t(|S_+| - 1)K^i_+)\)		
	\ENDFOR
	
	\STATE\(cons.set(idx(b), t(|S_+| - 1)\)
	
	\FOR{\((x_i = S_+.begin; x_i < S_+.end; x_i++)\)}
		\STATE \(cons.set(idx(\sigma^i_+), -p_i)\)
	\ENDFOR
	
	\STATE FeasSys.PushBack(cons, GreaterEqual, 0)
	
	\STATE cons.Clear()
	%adaug constrangerea pentru media pozitiva si deviatia medie pozitiva
	%  t(|S_+| - 1)\sum{\alpha_i K^i_+} + t(|S_+| - 1)b - \sum{\sigma^i_+ p_i}	
	\FOR{(i = 0; i < S.count; i++)}
		\STATE \(cons.set(idx(\alpha_i), -t(|S_-| - 1)K^i_-)\)
	\ENDFOR
	
	\STATE\(cons.set(idx(b), -t(|S_-| - 1)\)
	
	\FOR{\((x_i = S_-.begin; x_i < S_-.end; x_i ++)\)}
		\STATE \(cons.set(idx(\sigma^i_-), -p_i)\)
	\ENDFOR
	
	\STATE FeasSys.PushBack(cons, GreaterEqual, 0)
	
	\STATE delete cons
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}	
	
	\clearpage
	
	The last part constrains the averages to be strictly positive. Remember that this is a necessary condition without which the system would accept a null solution, making the classification meaningless. 
	We will express these two conditions by introducing an extra variable, \(\epsilon\), from which we will ask to be smaller than the averages, but positive. This has the benefit of keeping the system to the form \(Ax \geq 0\), which can then be rescalled as we see fit during the solving stage.
	
	\begin{algorithm}
	
	\caption{Feasibility System Construction Part 3}
	\label{feassystemconstruction3}		
	
	\begin{center}
	\begin{algorithmic}		
	
	\STATE cons = NewConstraint()
	
	\FOR {(i = 0; i < S.count; i++)}
		\STATE \(cons.set(idx(\alpha_i), K^i_+)\)
	\ENDFOR
	
	\STATE \(cons.set(idx(b), 1.0)\)
	\STATE \(cons.set(idx(\epsilon), -1.0)\)
	
	\STATE FeasSyS.PushBack(cons, GreaterEqual, 0)
	
	
	\STATE cons.Clear()
	
	\FOR{(i = 0; i < S.count; i++)}
		\STATE \(cons.set(idx(\alpha_i), -K^i_-)\)
	\ENDFOR
	
	\STATE \(cons.set(idx(b), -1.0)\)
	\STATE \(cons.set(idx(\epsilon), -1.0)\)
	
	\STATE FeasSys.PushBack(cons, GreaterEqual, 0)
	
	\STATE cons.Clear()
	
	\STATE \(cons.set(idx(\epsilon), 1.0)\)
	
	\STATE FeasSys.PushBack(cons, Greater, 0)
	
	\STATE delete cons
	
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}	
	\clearpage
			
\subsection{The Binary Search for Solving PVM}

	Solving the PVM classification problem consists of finding:
	
	\begin{enumerate} 
		\item \(t_{optimal}\in\mathbb{R}_+\) 
		\item \begin{itemize}
						\item \(\omega\in\mathbb{R}^n, b\in\mathbb{R}\) for non-kernel classification
						\item \(\alpha _i \in \mathbb{R}, i\in\overline{1..m}, b\in\mathbb{R}\) for kernel classification
					\end{itemize}
	\end{enumerate}
such that :
	
	\[
		\begin{array}{l}
		t_{optimal} = \\
		= inf\{t | Feas(t)\mbox{ is feasible}\} \\
		= sup\{t | Feas(t)\mbox{ is not feasible}\} \\
		\end{array}
	\]
	
	which gives the following algorithm for solving:
	
	\begin{algorithm}
	\caption{Binary Search Algorithm Used by \textit{PVM}}
	\label{algBinarySearch}
	\begin{center}
	\begin{algorithmic}
	
		\STATE \(t_{right} \gets 0\)
		\FOR{i = 0; i < \(i_{max}\); i++}
			\STATE \(t_{left}\gets t_{right}\)
			\STATE \(t_{right}\gets 2^i\)
			\IF{\(Feas(t_{right})\) is feasible}
				\STATE break
			\ENDIF
		\ENDFOR
		
		\IF{i == \(i_{max}\)}
			\RETURN sets{\_}are{\_}identical
		\ENDIF
		
		\WHILE{\(t_{right} - t_{left} > \epsilon\)}
			\STATE \(t_{center}\gets \frac{1}{2} (t_{left} + t_{right})\)
			\IF{\(Feast(t_{center})\) is feasible}
				\STATE \(t_{right}\gets t_{center}\)				
			\ELSE
				\STATE \(t_{left}\gets t_{center}\)
			\ENDIF			
		\ENDWHILE
		
		\STATE \(t_{optimal} = \frac{1}{2} (t_{left} + t_{right})\)	
	\end{algorithmic}
	\end{center}
	\end{algorithm}
\clearpage	
	
	Algorithm \ref{algBinarySearch} gives the value of \(t_{optimal}\) and the last successful resolution of \(Feas(t)\) gives the values for the sought variables.
	
	
\section{Solving the Feasibility Problem}

	As can be seen in \ref{pvm_feas}, the classification problem requires the solution to a set of linear feasibility problems (LFP), \(Feas(t)\).
	
	An LFP may be expressed as:	
	\[
		Ax \geq 0
	\]	
	where \(A\in M_{m\times n}(\mathbb{R})\) is the coefficients matrix and \(x\in \mathbb{R} ^n\) is the vector of unknowns.	
	\(A\) may also be viewed as \(A = \{a_i \in \mathbb{R}^n | i\in\overline{1..m}\}\). The LFP may also be expressed as :	
	\[
		<a_i, x> \geq 0 \mbox{ , where } i\in\overline{1..m}
	\]
	
	For such a matrix, we will denote by \(B = \{B_i|i\in\overline{1..k}\}\) a block cover for \(A\) :
	
	\begin{itemize}
		\item \(B_i = \{a_{i_0}, a_{i_1}, ... , a_{i_l} | a_{i_j}\in A\}\)
		\item \(\cup_{i=\overline{1..k}} B_i = A\)
	\end{itemize}
	
	Denote by \(Idx_i = \{i_0, i_1, ..., i_l\}\) the set of indices associated with block \(B_i\). 

	For a LFP with a block cover, \(B\), of the system matrix, we will denote by \(LFP_l\) the feasibility problem reduced to the \(l\)-th block:
	
	\[
		<a_i, x> \geq 0 \mbox{ , where } i\in Idx_l
	\]
	
	Denote by \(x_l\) the current solution to \(LFP_l\). 

	Pending the definition of the various procedures used, the pseudocode for solving the LFP is:
	
	\begin{algorithm}
	\caption{\textit{PVMFP}}
	\label{algPVMFP}
	\begin{center}
	\begin{algorithmic}
	
	\STATE \(iter\gets 0, x\gets 0\)
	\STATE \(flag{\_}cyclic \gets 0, flag{\_}infeasible \gets 0, flag{\_}solved \gets 0\)
	\STATE \(feasibility{\_}gap \gets \inf\)
	
	\WHILE{\(iter < iter_{max}\)}
		\STATE \(flag{\_}solved \gets 1\)
		\STATE \(feasibility{\_}gap \gets 0\)
		\FOR{\(l = 0; l < k; l++\)}
			\STATE PartiallySolveBlock(\(LFP_l\))
			\IF{\(LFP_l\) is infeasible}
				\STATE \(flag{\_}infeasible \gets 1\)
				\STATE break
			\ENDIF
			
			\IF{!(\(LFP_l\) is solved)}
				\STATE \(flag{\_}solved \gets 0\)
			\ELSE
				\STATE \(feasibility{\_}gap += LFP_l.feasibility{\_}gap\)
			\ENDIF								
		\ENDFOR
		
		\STATE \(x\gets ComponentAverage(x_0, x_1, ..., x_{k-1})\)
		
		\IF {flag{\_}infeasible || flag{\_}solved}
			\STATE break
		\ENDIF
		
		\IF{SolutionCycles()}
			\STATE \(flag{\_}cyclic \gets 1 \)
			\STATE break
		\ENDIF
		
		\STATE \(iter++\)
		
	\ENDWHILE
	
	
	\IF{\(flag{\_}infeasible || flag{\_}cyclic || iter == iter_{max}\)}
		\RETURN \(PVMFP\) is infeasible
	\ELSE
		\RETURN \(PVMFP\) is feasible
	\ENDIF
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}		
	
\clearpage		
	
	What is left is to define the various procedures used in the above pseudocode.
	
	
	\subsection{PartiallySolveBlock}
	
	
	\subsubsection{The Perceptron Method}
		This procedure is to be used on a single block of equations, \(B_l\). For ease of notation, we will refer to the equations 
in the block as \(A = \{a_i\in\mathbb{R} ^n | i\in\overline{1..m}\}\). The set of vectors will only refer to the set of vectors current in the block.

	The init procedure for the block will be : 
	%mcu: cred ca aici ti-a aranjat latex algoritmii in ordinea inversa
	
	\begin{algorithm}
	\caption{\textit{Block Init}}
	\label{algBlockInit}
	\begin{center}
	\begin{algorithmic}
	
		\STATE Input A
		\STATE rescale{\_}count = 0, \(R = I_n\), \(R^{-1} = I_n\)	
	
	\end{algorithmic}
	\end{center}
	\end{algorithm}			
	
	The PartillySolveBlock procedure:
	
	
	\begin{algorithm}
	\caption{\textit{PartiallySolveBlock}}
	\label{algPartialSolve}
	\begin{center}
	\begin{algorithmic}
	
	\STATE \textbf{INPUT :} \(x\in \mathbb{R}\) 	
	\STATE \textbf{OUTPUT :} \(x\in \mathbb{R}\), flag{\_}solve, flag{\_}infeasible, \(dist_{max}\in\mathbb{R}_+\)
	
	\STATE iter{\_}count = 0, \(\sigma = \frac{1}{32n}\)
	\STATE \(x_r = R^{-1} x\), \(A_r = A\cdot R\)
	
	\WHILE{\(iter{\_}count < iter_{max}\)}
	\STATE
	\STATE \textit{Perceptron Phase}	
	\FOR {i = 0; i < \(\frac{1}{\sigma^2}; i++\)}
		\FOR {j = 0; j < \(A_r\).count; j++}
			\IF {\(<A_r(j), x_r> \leq 0\)}
				\STATE \(x_r += \overline{A_r(j)}\)
				\STATE break
			\ENDIF
		\ENDFOR			
	\ENDFOR
	
	\IF{\(A_r \cdot x_r \geq 0\)}
		\RETURN \(R\cdot x_r\)
	\ENDIF
	\STATE
	\STATE \textit{Perceptron Improvement}
	
	\LOOP	
	\STATE Choose \(x_{imp}\in\mathbb{S}^n\) uniformly random
	
	\FOR {i = 0; i < \(\frac{\ln n}{\sigma^2}; i++\)}
		\FOR {j = 0; j < \(A_r\).count; j++}
			\IF {\(<A_r(j), x_r> < -\sigma\)} %mcu: cine este acest '-\sigma' ? -> este constanta initializata la inceputul algoritmului cu \frac{1}{32n}
				\STATE \(x_{imp} -= (\overline{A_r(j)}\cdot \overline{x_{imp}})x_{imp} \ \)
				\STATE break
			\ENDIF
		\ENDFOR
	\ENDFOR
	
	\STATE \(flag{\_}repeat \gets 0\)
	
	\FOR{j = 0; j < \(A_r\).count; j++}
			\IF {\(<A_r(j), x_r> < -\sigma\)}
				\STATE \(flag{\_}repeat \gets 1\)
				\STATE break
			\ENDIF		
	\ENDFOR
	
	\IF{!flag{\_}repeat}
		\STATE break
	\ENDIF	
		
	\ENDLOOP
	
	\STATE
	\STATE \textit{Scaling}	
	
	\STATE \(R_{iter{\_}count} = I_n + \overline{x_{imp}} \cdot \overline{x_{imp}}^T \)
	\STATE \(R^{-1}_{iter{\_}count} = I_n - \frac{1}{2} \overline{x_{imp}} \cdot \overline{x_{imp}}^T \)
	
	\STATE \(R \gets R \cdot R_{iter{\_}count}\)
	\STATE \(R^{-1} \gets R^{-1}_{iter{\_}count}\cdot R^{-1}\)
	\STATE \(A_r \gets A_r \cdot R_{iter{\_}count}\)
	\STATE \(x_r \gets R^{-1}_{iter{\_}count} \cdot x_r\)
	
	
	\STATE \(iter{\_}count++\)
	
	\ENDWHILE
	\end{algorithmic}
	\end{center}
	\end{algorithm}		
	
\clearpage	

	\subsection{The Window Averaged Update Method}
	
	Unlike the previous perceptron method, this method only requires typical solution updates, without any rescaling of the constraints.
	
	We first define a set of sub-blocks, which we will also refer to as windows of equations : 
	
	\begin{itemize}
		\item A window, \(W_i\), will be defined as \(W_i = \{a_i^j | j\in \overline{0..k_i}\}\).
		\item The join of all windows in a block will be the block itself : \(\cup_{i} W_i = A\).
		\item The windows will not be necessarily disjoint.
	\end{itemize}
	
	An iteration over the current block represents an iterative update for each window (sub-block) associated to it. All that is left to be done is 
to show how an update corresponding to a window is done. With the exception of the block which contains the final four equations, all other equations
help define majorants for the deviation from the average associated with a record, so we will indicate the particular way in which these equations impose updates.

	Let \(W = \{a_0, a_1, ..., a_n\}\) denote the current window.  The update associated to this window is given by an affine combination of the updates suggestested 
by each of its unsatisfied constraints.
	
	There are two types of constraints : 
	
	\begin{itemize}
		\item Constraints with a 0 RHS : \(<a,x> \geq 0\)
		\item Constraints with a 1 RHS : \(<a,x> \geq 1\)
	\end{itemize}
	
	If a constraint is not satisfied, it will suggest the following update :
	\begin{itemize}
		\item 0 RHS : \(u = \frac{-<a,x>}{||a||^2}a\)
		\item 1 RHS : \(u = \frac{1 - <a,x>}{||a||^2}a\)
	\end{itemize}	
	where \(||a||\) denotes the norm of the constraint, which, for \(a = (a^0, a^1, \ldots, a^n)\) is 
	\[
		||a|| = \sqrt{(a^0)^2 + (a^n)^2 + \ldots (a^n)^2} = \sqrt{\sum_i  {(a^i)^2}}
	\]
	
	Note that the suggested update would take the current solution to its projection on the constraint hyperplane.
	
	Foar an equation which is not satisfied, we define its score as :
	\begin{itemize}
		\item 0 RHS : \(S(a) = frac{-<a, x>}{||a||}\)
		\item 1 RHS : \(S(a) = frac{1 - <a, x>}{||a||}\)
	\end{itemize}
	
	Now consider the following :
	
	\begin{itemize}
		\item \(a_{n_1}, a_{n_2},\ldots, a_{n_k}\) the constraints in the current window that are not satisfied
		\item \(u(a_{n_1}), u(a_{n_2}),\ldots, u(a_{n_k})\) the updates suggested by every constraint that is not satisfied
		\item \(S(a_{n_1}), S(a_{n_2}),\ldots, S(a_{n_k})\) the scores associated to each such constraint
		\item \(p_{n_1}, p_{n_2},\ldots,p_{n_k}\), where \(p_{n_i} = \frac{S(a_{n_i})}{\sum_j {S(a_{n_j})}}\)
	\end{itemize}

	Then the update for the current window will be:	
	\[
		u = \lambda_{iter} (p_{n_1} \cdot u(a_{n_1}) + p_{n_2} \cdot u(a_{n_2}) + ... + p_{n_k} \cdot u(a_{n_k})) = \lambda_{iter} \sum_i {p_{n_i} \cdot u(a_{n_i})}
	\]
	where \(\lambda_{iter}\in [\delta, 2-\delta]\), \(\delta > 0\), is a relaxation parameter.
	
	Suppose the constraints within the block are simply constraints which define the deviation associated to a record (like almost all the equations
in our system), then we can detail the procedure even further. 
	
	Let \(W = \{x_0, x_1, \ldots, x_k\}\) the records in the current window. For each such record, there would be 2 constraints helping define its deviation:
	
	\begin{itemize}
		\item If \(x_j\) has a positive label : 
			\begin{itemize}
				\item Define its norm as : \(norm_j = 1 + \sum_{x_i\in S} {(K(x_i, x_j) - K^i_+)^2}\)
				\item \(\sigma^j_+ - \sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)} \geq 0\)
				\item \(\sigma^j_+ + \sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)} \geq 0\)
			\end{itemize}
			
		\item If \(x_j\) has a negative label : 
			\begin{itemize}
				\item Define its norm as : \(norm_j = 1 + \sum_{x_i\in S} {(K(x_i, x_j) - K^i_-)^2}\)
				\item \(\sigma^j_- - \sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_-)} \geq 0\)
				\item \(\sigma^j_- + \sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_-)} \geq 0\)
			\end{itemize}			
	\end{itemize}
	
	Note that the norms may be precomputed, as they do not change in the course of the algorithm.
	
	
	We can check whether the equations associated are satisfied by computing:
	
	\begin{itemize}
		\item For \(x_j\) with a positive label : 
			\begin{enumerate}
				\item Compute \(s_j = \sum_{x_i\in S} {\alpha_i (K(x_i, x_j) - K^i_+)}\)
				\item If \(\sigma^j_+ - s_j < 0\), then record the suggested update 
							\[u_{j_0} = \frac{s_j - \sigma^j_+}{norm_j^2} \cdot (K(x_0, x_j) - K^0_+, K(x_1, x_j) - K^1_+, \ldots, K(x_n, x_j) - K^n_+, 0, 0, \ldots, 1, 0, \ldots, 0)\]							
							with score \(S_{j_0} = \frac{s_j - \sigma^j_+}{norm_j}\), where the 1 corresponds to the position of \(\sigma^j_+\).
							
				\item If \(\sigma^j_+ + s_j < 0\), then record the suggested update 
							\[u_{j_1} = \frac{- s_j - \sigma^j_+}{norm_j^2} \cdot (K^0_+ - K(x_0, x_j), K^1_+ - K(x_1, x_j), \ldots, K^n_+ - K(x_n, x_j), 0, 0, \ldots, 1, 0, \ldots, 0)\]
							with score \(S_{j_1} = \frac{- s_j - \sigma^j_+}{norm_j}\), where the \(1\) corresponds to the position of \(\sigma^j_+\).
			\end{enumerate}
			
		\item For \(x_j\) with a negative label just change \(\sigma^j_+\) into \(\sigma^j_-\) and \(K^i_+\) into \(K^i_-\) in the above.
	\end{itemize}
	
	Note that, by controlling the size of the update window, we may significantly change the way the algorithm works. 
	As such, when the window size is minimal, which is to say it is 1, the local algorithm in fact becomes the KACZ algorithm, 
updating the solution iteratively according to each unsatisfied equation. The overall algorithm then becomes identical to the CARP algorithm.

	When the window size is maximal, which is to say equal to the size of the block, the local algorithm becomes a derivative of the Cimmino, 
with the convex weights used for obtaining the update given by the scores of the equations.

	Also note that the processing required for obtaining the update for one window may be done completely parallel, as the suggested updates 
associated with each unsatisfied equation may be done completely independent from one another. 
	
	\clearpage
		
	\subsection{ComponentAverage}
	
		Let \(x_0, ..., x_k\) be the partial solutions outputed by the PartiallySolveBlock procedure for every block, \(B_0, ..., B_k\). 

		If \(x_l = (x^0_l, ..., x^{n-1}_l)\), we will denote by \(SI(i) = \{idx_t| idx_t \in\overline{0..k}\}\) the set of indices for which the \(i\)-th component of the 
point in \(\mathbb{R} ^n\) has at least an equation in block \(B_{idx_t}\) that has a nonzero coefficient corresponding to that \(i\)-th component. \(SI(i)\) tracks all the 
blocks that influence component \(i\).

	The input/output for the ComponentAverage procedure is :
	
	\begin{itemize}
		\item \textbf{Input} The partial solutions \(x_0, ..., x_k \in \mathbb{R} ^n\)
		\item \textbf{Output} The current overall iterate \(x = (x^0, x^1,..., x^{n-1})\in \mathbb{R} ^n\)
	\end{itemize}
	
	The ComponentAverage procedure is:
	
	\begin{enumerate}
		\item \(x^i = \frac{1}{|SI(i)|} \cdot \sum_{j\in SI(i)} {x^i_j}\)
		\item Output \(x = (x^0, x^1, ..., x^{n-1})\)
	\end{enumerate}		
	
	
	Simply put, the ComponentAverage computes the average of a coordonate only over the blocks that influence that coordonate.
		
	\subsection{SolutionCycles}
	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BIBLIOGRAPHY AND OTHER LISTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% A small distance to the other stuff in the table of contents (toc)
\addtocontents{toc}{\protect\vspace*{\baselineskip}}

%% The Bibliography
%% ==> You need a file 'literature.bib' for this.
%% ==> You need to run BibTeX for this (Project | Properties... | Uses BibTeX)
%\addcontentsline{toc}{chapter}{Bibliography} %'Bibliography' into toc
%\nocite{*} %Even non-cited BibTeX-Entries will be shown.
%\bibliographystyle{alpha} %Style of Bibliography: plain / apalike / amsalpha / ...
%\bibliography{literature} %You need a file 'literature.bib' for this.

%% The List of Figures
\clearpage
\addcontentsline{toc}{chapter}{List of Figures}
\listoffigures

%% The List of Tables
\clearpage
\addcontentsline{toc}{chapter}{List of Tables}
\listoftables


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% APPENDICES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
%% ==> Write your text here or include other files.

%\input{FileName} %You need a file 'FileName.tex' for this.


\end{document}

